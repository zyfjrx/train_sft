{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd16f56-8cb4-43a8-b529-98fb19a544b8",
   "metadata": {},
   "source": [
    "将conda环境注册到内核中\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name evalscope --display-name \"Python evalscope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5c61ba-b2b5-4cd4-827b-7b3a4cc06530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalscope import TaskConfig, run_task\n",
    "task_cfg = TaskConfig(\n",
    "    model='Qwen3-8B-sft-all', #与vllm启动时指定的模型名一致\n",
    "    api_url='http://127.0.0.1:8000/v1/chat/completions',\n",
    "    eval_type='service',\n",
    "    datasets=[\n",
    "        'data_collection',\n",
    "    ],\n",
    "    dataset_args={\n",
    "        'data_collection': {\n",
    "            'dataset_id': 'modelscope/EvalScope-Qwen3-Test',\n",
    "            'filters': {'remove_until': '</think>'}  # 过滤掉思考的内容\n",
    "        }\n",
    "    },\n",
    "    eval_batch_size=128,\n",
    "    generation_config={\n",
    "        'max_tokens': 30000,  # 最大生成token数，建议设置为较大值避免输出截断\n",
    "        'temperature': 0.6,  # 采样温度 (qwen 报告推荐值)\n",
    "        'top_p': 0.95,  # top-p采样 (qwen 报告推荐值)\n",
    "        'top_k': 20,  # top-k采样 (qwen 报告推荐值)\n",
    "        'n': 1,  # 每个请求产生的回复数量\n",
    "    },\n",
    "    timeout=60000,  # 超时时间\n",
    "    stream=True,  # 是否使用流式输出\n",
    "    limit=200,  # 设置为200条数据进行测试\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4623eaa2-8253-4141-acd0-01c4e07656db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 04:23:27,915 - evalscope - INFO - Args: Task config is provided with TaskConfig type.\n",
      "2025-08-20 04:23:27,918 - evalscope - INFO - Loading dataset from modelscope: > dataset_name: modelscope/EvalScope-Qwen3-Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset to directory: /root/.cache/modelscope/hub/datasets/modelscope/EvalScope-Qwen3-Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 04:23:30,927 - evalscope - INFO - Dump task config to ./outputs/20250820_042327/configs/task_config_7d5c0b.yaml\n",
      "2025-08-20 04:23:30,931 - evalscope - INFO - {\n",
      "    \"model\": \"Qwen3-8B-sft-all\",\n",
      "    \"model_id\": \"Qwen3-8B-sft-all\",\n",
      "    \"model_args\": {},\n",
      "    \"model_task\": \"text_generation\",\n",
      "    \"template_type\": null,\n",
      "    \"chat_template\": null,\n",
      "    \"datasets\": [\n",
      "        \"data_collection\"\n",
      "    ],\n",
      "    \"dataset_args\": {\n",
      "        \"data_collection\": {\n",
      "            \"dataset_id\": \"modelscope/EvalScope-Qwen3-Test\",\n",
      "            \"filters\": {\n",
      "                \"remove_until\": \"</think>\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"dataset_dir\": \"/root/.cache/modelscope/hub/datasets\",\n",
      "    \"dataset_hub\": \"modelscope\",\n",
      "    \"generation_config\": {\n",
      "        \"max_tokens\": 30000,\n",
      "        \"temperature\": 0.6,\n",
      "        \"top_p\": 0.95,\n",
      "        \"top_k\": 20,\n",
      "        \"n\": 1\n",
      "    },\n",
      "    \"eval_type\": \"service\",\n",
      "    \"eval_backend\": \"Native\",\n",
      "    \"eval_config\": null,\n",
      "    \"stage\": \"all\",\n",
      "    \"limit\": 200,\n",
      "    \"eval_batch_size\": 128,\n",
      "    \"mem_cache\": false,\n",
      "    \"use_cache\": null,\n",
      "    \"work_dir\": \"./outputs/20250820_042327\",\n",
      "    \"outputs\": null,\n",
      "    \"ignore_errors\": false,\n",
      "    \"debug\": false,\n",
      "    \"dry_run\": false,\n",
      "    \"seed\": 42,\n",
      "    \"api_url\": \"http://127.0.0.1:8000/v1/chat/completions\",\n",
      "    \"api_key\": \"EMPTY\",\n",
      "    \"timeout\": 60000,\n",
      "    \"stream\": true,\n",
      "    \"judge_strategy\": \"auto\",\n",
      "    \"judge_worker_num\": 1,\n",
      "    \"judge_model_args\": {},\n",
      "    \"analysis_report\": false\n",
      "}\n",
      "Getting answers: 100%|██████████| 200/200 [00:53<00:00,  3.71it/s]\n",
      "Getting reviews:  37%|███▋      | 74/200 [00:02<00:03, 38.31it/s]2025-08-20 04:24:27,019 - evalscope - ERROR - Error getting review for sample index 18128: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/envs/evalscope/nltk_data'\n",
      "    - '/root/miniconda3/envs/evalscope/share/nltk_data'\n",
      "    - '/root/miniconda3/envs/evalscope/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      ". Skipping this sample.\n",
      "Getting reviews: 100%|██████████| 200/200 [00:08<00:00, 23.76it/s]\n",
      "Getting scores: 100%|██████████| 200/200 [00:00<00:00, 130806.30it/s]\n",
      "2025-08-20 04:24:33,314 - evalscope - INFO - subset_level Report:\n",
      "+-------------+-------------------------+-----------------+-------------------------------------+---------------+-------+\n",
      "|  task_type  |         metric          |  dataset_name   |             subset_name             | average_score | count |\n",
      "+-------------+-------------------------+-----------------+-------------------------------------+---------------+-------+\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |                math                 |    0.6471     |  17   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |               health                |    0.2727     |  11   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |             engineering             |    0.3636     |  11   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |                 law                 |    0.1818     |  11   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |               physics               |    0.6364     |  11   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |             psychology              |      0.4      |  10   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |               biology               |      0.6      |  10   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |              chemistry              |      0.5      |   8   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |                other                |     0.125     |   8   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |              economics              |    0.4286     |   7   |\n",
      "| instruction | prompt_level_strict_acc |     ifeval      |               default               |      0.5      |   6   |\n",
      "| instruction | prompt_level_loose_acc  |     ifeval      |               default               |    0.6667     |   6   |\n",
      "| instruction |  inst_level_strict_acc  |     ifeval      |               default               |      0.5      |   6   |\n",
      "| instruction |  inst_level_loose_acc   |     ifeval      |               default               |    0.6667     |   6   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |              business               |      0.8      |   5   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |             philosophy              |     0.75      |   4   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |          computer science           |      1.0      |   4   |\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |               history               |     0.25      |   4   |\n",
      "|  knowledge  |      AveragePass@1      |      gpqa       |            gpqa_diamond             |    0.6667     |   3   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |      high_school_world_history      |    0.3333     |   3   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |             prehistory              |    0.3333     |   3   |\n",
      "|    code     |         Pass@1          | live_code_bench |                v5_v6                |      0.0      |   2   |\n",
      "|    math     |      AveragePass@1      |    math_500     |               Level 5               |      0.5      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |              virology               |      0.5      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |     high_school_macroeconomics      |      0.5      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |          medical_genetics           |      1.0      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |       high_school_psychology        |      0.5      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |          security_studies           |      1.0      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |          us_foreign_policy          |      0.5      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |             philosophy              |      1.0      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |       elementary_mathematics        |      1.0      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |           world_religions           |      1.0      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |      college_computer_science       |      1.0      |   2   |\n",
      "|    exam     |     AverageAccuracy     |      iquiz      |                 EQ                  |      1.0      |   2   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |       high_school_mathematics       |      0.5      |   2   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |             art_studies             |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |            civil_servant            |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |           basic_medicine            |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |         high_school_biology         |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |          education_science          |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |         conceptual_physics          |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |         college_mathematics         |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |          college_chemistry          |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |           business_ethics           |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |               anatomy               |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |          abstract_algebra           |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |         veterinary_medicine         |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |           tax_accountant            |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |           sports_science            |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |              physician              |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |        middle_school_history        |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |               marxism               |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |                 law                 |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |         high_school_history         |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |       electrical_engineering        |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |            econometrics             |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |              nutrition              |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |           moral_scenarios           |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |             human_aging             |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |           human_sexuality           |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |            formal_logic             |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    | high_school_government_and_politics |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |         high_school_biology         |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |        high_school_chemistry        |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |            miscellaneous            |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |             management              |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |              sociology              |      1.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |        professional_medicine        |      0.0      |   1   |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |       professional_accounting       |      1.0      |   1   |\n",
      "|    math     |      AveragePass@1      |    math_500     |               Level 3               |      1.0      |   1   |\n",
      "+-------------+-------------------------+-----------------+-------------------------------------+---------------+-------+\n",
      "2025-08-20 04:24:33,315 - evalscope - INFO - dataset_level Report:\n",
      "+-------------+-------------------------+-----------------+---------------+-------+\n",
      "|  task_type  |         metric          |  dataset_name   | average_score | count |\n",
      "+-------------+-------------------------+-----------------+---------------+-------+\n",
      "|    exam     |     AverageAccuracy     |    mmlu_pro     |    0.4711     |  121  |\n",
      "|    exam     |     AverageAccuracy     |   mmlu_redux    |    0.6939     |  49   |\n",
      "|    exam     |     AverageAccuracy     |      ceval      |    0.6154     |  13   |\n",
      "| instruction | prompt_level_loose_acc  |     ifeval      |    0.6667     |   6   |\n",
      "| instruction | prompt_level_strict_acc |     ifeval      |      0.5      |   6   |\n",
      "| instruction |  inst_level_loose_acc   |     ifeval      |    0.6667     |   6   |\n",
      "| instruction |  inst_level_strict_acc  |     ifeval      |      0.5      |   6   |\n",
      "|  knowledge  |      AveragePass@1      |      gpqa       |    0.6667     |   3   |\n",
      "|    math     |      AveragePass@1      |    math_500     |    0.6667     |   3   |\n",
      "|    exam     |     AverageAccuracy     |      iquiz      |      1.0      |   2   |\n",
      "|    code     |         Pass@1          | live_code_bench |      0.0      |   2   |\n",
      "+-------------+-------------------------+-----------------+---------------+-------+\n",
      "2025-08-20 04:24:33,316 - evalscope - INFO - task_level Report:\n",
      "+-------------+-------------------------+---------------+-------+\n",
      "|  task_type  |         metric          | average_score | count |\n",
      "+-------------+-------------------------+---------------+-------+\n",
      "|    exam     |     AverageAccuracy     |    0.5459     |  185  |\n",
      "| instruction |  inst_level_loose_acc   |    0.6667     |   6   |\n",
      "| instruction |  inst_level_strict_acc  |      0.5      |   6   |\n",
      "| instruction | prompt_level_loose_acc  |    0.6667     |   6   |\n",
      "| instruction | prompt_level_strict_acc |      0.5      |   6   |\n",
      "|  knowledge  |      AveragePass@1      |    0.6667     |   3   |\n",
      "|    math     |      AveragePass@1      |    0.6667     |   3   |\n",
      "|    code     |         Pass@1          |      0.0      |   2   |\n",
      "+-------------+-------------------------+---------------+-------+\n",
      "2025-08-20 04:24:33,317 - evalscope - INFO - tag_level Report:\n",
      "+------+-------------------------+---------------+-------+\n",
      "| tags |         metric          | average_score | count |\n",
      "+------+-------------------------+---------------+-------+\n",
      "|  en  |     AverageAccuracy     |    0.5353     |  170  |\n",
      "|  zh  |     AverageAccuracy     |    0.6667     |  15   |\n",
      "|  en  |  inst_level_loose_acc   |    0.6667     |   6   |\n",
      "|  en  |      AveragePass@1      |    0.6667     |   6   |\n",
      "|  en  | prompt_level_loose_acc  |    0.6667     |   6   |\n",
      "|  en  |  inst_level_strict_acc  |      0.5      |   6   |\n",
      "|  en  | prompt_level_strict_acc |      0.5      |   6   |\n",
      "|  en  |         Pass@1          |      0.0      |   2   |\n",
      "+------+-------------------------+---------------+-------+\n",
      "2025-08-20 04:24:33,319 - evalscope - INFO - category_level Report:\n",
      "+-----------+--------------+-------------------------+---------------+-------+\n",
      "| category0 |  category1   |         metric          | average_score | count |\n",
      "+-----------+--------------+-------------------------+---------------+-------+\n",
      "|   Qwen3   |   English    |     AverageAccuracy     |    0.5353     |  170  |\n",
      "|   Qwen3   |   Chinese    |     AverageAccuracy     |    0.6667     |  15   |\n",
      "|   Qwen3   |   English    |  inst_level_loose_acc   |    0.6667     |   6   |\n",
      "|   Qwen3   |   English    |  inst_level_strict_acc  |      0.5      |   6   |\n",
      "|   Qwen3   |   English    | prompt_level_strict_acc |      0.5      |   6   |\n",
      "|   Qwen3   |   English    | prompt_level_loose_acc  |    0.6667     |   6   |\n",
      "|   Qwen3   | Math&Science |      AveragePass@1      |    0.6667     |   6   |\n",
      "|   Qwen3   |     Code     |         Pass@1          |      0.0      |   2   |\n",
      "+-----------+--------------+-------------------------+---------------+-------+\n",
      "2025-08-20 04:24:33,330 - evalscope - INFO - Skipping report analysis (`analysis_report=False`).\n",
      "2025-08-20 04:24:33,333 - evalscope - INFO - Report saved to ./outputs/20250820_042327/reports/Qwen3-8B-sft-all/EvalScope-Qwen3-Test.json\n",
      "2025-08-20 04:24:33,346 - evalscope - INFO - Overall report table: \n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Model            | Dataset              | Metric   | Subset                                         |   Num |   Score | Cat.0   | Cat.1        |\n",
      "+==================+======================+==========+================================================+=======+=========+=========+==============+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/art_studies                              |     1 |  0      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/basic_medicine                           |     1 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/civil_servant                            |     1 |  0      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/education_science                        |     1 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/high_school_biology                      |     1 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/high_school_history                      |     1 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/law                                      |     1 |  0      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/marxism                                  |     1 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/middle_school_history                    |     1 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/physician                                |     1 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/sports_science                           |     1 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/tax_accountant                           |     1 |  0      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ceval/veterinary_medicine                      |     1 |  0      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | iquiz/EQ                                       |     2 |  1      | Qwen3   | Chinese      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | live_code_bench/v5_v6                          |     2 |  0      | Qwen3   | Code         |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | ifeval/default                                 |    24 |  0.5833 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/biology                               |    10 |  0.6    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/business                              |     5 |  0.8    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/chemistry                             |     8 |  0.5    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/computer science                      |     4 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/economics                             |     7 |  0.4286 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/engineering                           |    11 |  0.3636 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/health                                |    11 |  0.2727 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/history                               |     4 |  0.25   | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/law                                   |    11 |  0.1818 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/math                                  |    17 |  0.6471 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/other                                 |     8 |  0.125  | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/philosophy                            |     4 |  0.75   | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/physics                               |    11 |  0.6364 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_pro/psychology                            |    10 |  0.4    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/abstract_algebra                    |     1 |  0      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/anatomy                             |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/business_ethics                     |     1 |  0      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/college_chemistry                   |     1 |  0      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/college_computer_science            |     2 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/college_mathematics                 |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/conceptual_physics                  |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/econometrics                        |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/electrical_engineering              |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/elementary_mathematics              |     2 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/formal_logic                        |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/high_school_biology                 |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/high_school_chemistry               |     1 |  0      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/high_school_government_and_politics |     1 |  0      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/high_school_macroeconomics          |     2 |  0.5    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/high_school_mathematics             |     2 |  0.5    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/high_school_psychology              |     2 |  0.5    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/high_school_world_history           |     3 |  0.3333 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/human_aging                         |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/human_sexuality                     |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/management                          |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/medical_genetics                    |     2 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/miscellaneous                       |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/moral_scenarios                     |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/nutrition                           |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/philosophy                          |     2 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/prehistory                          |     3 |  0.3333 | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/professional_accounting             |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/professional_medicine               |     1 |  0      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/security_studies                    |     2 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/sociology                           |     1 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/us_foreign_policy                   |     2 |  0.5    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/virology                            |     2 |  0.5    | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | mmlu_redux/world_religions                     |     2 |  1      | Qwen3   | English      |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | gpqa/gpqa_diamond                              |     3 |  0.6667 | Qwen3   | Math&Science |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | math_500/Level 3                               |     1 |  1      | Qwen3   | Math&Science |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | math_500/Level 5                               |     2 |  0.5    | Qwen3   | Math&Science |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+\n",
      "| Qwen3-8B-sft-all | EvalScope-Qwen3-Test | Average  | OVERALL                                        |   217 |  0.5484 | -       |              |\n",
      "+------------------+----------------------+----------+------------------------------------------------+-------+---------+---------+--------------+ \n",
      "\n",
      "2025-08-20 04:24:33,347 - evalscope - INFO - Finished evaluation for Qwen3-8B-sft-all on ['data_collection']\n",
      "2025-08-20 04:24:33,347 - evalscope - INFO - Output directory: ./outputs/20250820_042327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EvalScope-Qwen3-Test': Report(name='data_collection', dataset_name='EvalScope-Qwen3-Test', dataset_pretty_name='', dataset_description='', model_name='Qwen3-8B-sft-all', score=0.5484, metrics=[Metric(name='Average', num=217, score=0.5484, macro_score=0.4687, categories=[Category(name=('Qwen3', 'Chinese'), num=15, score=0.6667, macro_score=0.6429, subsets=[Subset(name='ceval/art_studies', score=0.0, num=1), Subset(name='ceval/basic_medicine', score=1.0, num=1), Subset(name='ceval/civil_servant', score=0.0, num=1), Subset(name='ceval/education_science', score=1.0, num=1), Subset(name='ceval/high_school_biology', score=1.0, num=1), Subset(name='ceval/high_school_history', score=1.0, num=1), Subset(name='ceval/law', score=0.0, num=1), Subset(name='ceval/marxism', score=1.0, num=1), Subset(name='ceval/middle_school_history', score=1.0, num=1), Subset(name='ceval/physician', score=1.0, num=1), Subset(name='ceval/sports_science', score=1.0, num=1), Subset(name='ceval/tax_accountant', score=0.0, num=1), Subset(name='ceval/veterinary_medicine', score=0.0, num=1), Subset(name='iquiz/EQ', score=1.0, num=2)]), Category(name=('Qwen3', 'Code'), num=2, score=0.0, macro_score=0.0, subsets=[Subset(name='live_code_bench/v5_v6', score=0.0, num=2)]), Category(name=('Qwen3', 'English'), num=194, score=0.5412, macro_score=0.647, subsets=[Subset(name='ifeval/default', score=0.5833, num=24), Subset(name='mmlu_pro/biology', score=0.6, num=10), Subset(name='mmlu_pro/business', score=0.8, num=5), Subset(name='mmlu_pro/chemistry', score=0.5, num=8), Subset(name='mmlu_pro/computer science', score=1.0, num=4), Subset(name='mmlu_pro/economics', score=0.4286, num=7), Subset(name='mmlu_pro/engineering', score=0.3636, num=11), Subset(name='mmlu_pro/health', score=0.2727, num=11), Subset(name='mmlu_pro/history', score=0.25, num=4), Subset(name='mmlu_pro/law', score=0.1818, num=11), Subset(name='mmlu_pro/math', score=0.6471, num=17), Subset(name='mmlu_pro/other', score=0.125, num=8), Subset(name='mmlu_pro/philosophy', score=0.75, num=4), Subset(name='mmlu_pro/physics', score=0.6364, num=11), Subset(name='mmlu_pro/psychology', score=0.4, num=10), Subset(name='mmlu_redux/abstract_algebra', score=0.0, num=1), Subset(name='mmlu_redux/anatomy', score=1.0, num=1), Subset(name='mmlu_redux/business_ethics', score=0.0, num=1), Subset(name='mmlu_redux/college_chemistry', score=0.0, num=1), Subset(name='mmlu_redux/college_computer_science', score=1.0, num=2), Subset(name='mmlu_redux/college_mathematics', score=1.0, num=1), Subset(name='mmlu_redux/conceptual_physics', score=1.0, num=1), Subset(name='mmlu_redux/econometrics', score=1.0, num=1), Subset(name='mmlu_redux/electrical_engineering', score=1.0, num=1), Subset(name='mmlu_redux/elementary_mathematics', score=1.0, num=2), Subset(name='mmlu_redux/formal_logic', score=1.0, num=1), Subset(name='mmlu_redux/high_school_biology', score=1.0, num=1), Subset(name='mmlu_redux/high_school_chemistry', score=0.0, num=1), Subset(name='mmlu_redux/high_school_government_and_politics', score=0.0, num=1), Subset(name='mmlu_redux/high_school_macroeconomics', score=0.5, num=2), Subset(name='mmlu_redux/high_school_mathematics', score=0.5, num=2), Subset(name='mmlu_redux/high_school_psychology', score=0.5, num=2), Subset(name='mmlu_redux/high_school_world_history', score=0.3333, num=3), Subset(name='mmlu_redux/human_aging', score=1.0, num=1), Subset(name='mmlu_redux/human_sexuality', score=1.0, num=1), Subset(name='mmlu_redux/management', score=1.0, num=1), Subset(name='mmlu_redux/medical_genetics', score=1.0, num=2), Subset(name='mmlu_redux/miscellaneous', score=1.0, num=1), Subset(name='mmlu_redux/moral_scenarios', score=1.0, num=1), Subset(name='mmlu_redux/nutrition', score=1.0, num=1), Subset(name='mmlu_redux/philosophy', score=1.0, num=2), Subset(name='mmlu_redux/prehistory', score=0.3333, num=3), Subset(name='mmlu_redux/professional_accounting', score=1.0, num=1), Subset(name='mmlu_redux/professional_medicine', score=0.0, num=1), Subset(name='mmlu_redux/security_studies', score=1.0, num=2), Subset(name='mmlu_redux/sociology', score=1.0, num=1), Subset(name='mmlu_redux/us_foreign_policy', score=0.5, num=2), Subset(name='mmlu_redux/virology', score=0.5, num=2), Subset(name='mmlu_redux/world_religions', score=1.0, num=2)]), Category(name=('Qwen3', 'Math&Science'), num=6, score=0.6667, macro_score=0.7222, subsets=[Subset(name='gpqa/gpqa_diamond', score=0.6667, num=3), Subset(name='math_500/Level 3', score=1.0, num=1), Subset(name='math_500/Level 5', score=0.5, num=2)])])], analysis='N/A')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_task(task_cfg=task_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c917f-f662-48ef-bb89-f322ed5cae2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python evalscope",
   "language": "python",
   "name": "evalscope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
